// AudioRecorder.tsx
// ğŸ“Œ ì»´í¬ë„ŒíŠ¸ ì „ì²´ì˜ ì—­í• : ë§ˆì´í¬ ê¶Œí•œ í™•ì¸, ìŒì„± ë…¹ìŒ ë° ìŒì„± ì¸ì‹, ë…¹ìŒ ì¤‘ì§€, ì‹¤ì‹œê°„ UI ìƒíƒœ ë Œë”ë§ì„ ë‹´ë‹¹

import React, { useState, useRef, useEffect, useCallback } from 'react';
import { AudioRecorderProps } from './AudioRecorderProps'; // ğŸ¯ props íƒ€ì… ë¶„ë¦¬
import { SpeechRecognition, SpeechRecognitionErrorEvent, SpeechRecognitionEvent } from './SpeechRecognitionTypes'; // ğŸ¯ Web Speech API ê´€ë ¨ íƒ€ì…
import AudioVisualizer from './AudioVisualizer'; // ğŸ¯ ì˜¤ë””ì˜¤ ë ˆë²¨ ì‹œê°í™” ì»´í¬ë„ŒíŠ¸

const AudioRecorder: React.FC<AudioRecorderProps> = ({
  onRecordingComplete,
  isRecording,
  onRecordingStart,
  onRecordingStop,
  continuousMode = false
}) => {
  // ğŸ§  ìƒíƒœ ì •ì˜
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [microphoneAccess, setMicrophoneAccess] = useState<'granted' | 'denied' | 'pending'>('pending');
  const [audioLevel, setAudioLevel] = useState<number>(0);
  const [isProcessing, setIsProcessing] = useState(false);

  // ğŸ§  ì°¸ì¡° ì •ì˜
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const speechRecognitionRef = useRef<SpeechRecognition | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const durationTimerRef = useRef<NodeJS.Timeout | null>(null);
  const isStoppingRef = useRef<boolean>(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioLevelTimerRef = useRef<NodeJS.Timeout | null>(null);

  // âœ… ê¶Œí•œ í™•ì¸ í•¨ìˆ˜
  const checkMicrophonePermission = useCallback(async () => {
    try {
      const result = await navigator.permissions.query({ name: 'microphone' as PermissionName });
      if (result.state === 'granted') {
        setMicrophoneAccess('granted');
        return true;
      } else if (result.state === 'denied') {
        setMicrophoneAccess('denied');
        return false;
      } else {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          stream.getTracks().forEach(track => track.stop());
          setMicrophoneAccess('granted');
          return true;
        } catch {
          setMicrophoneAccess('denied');
          return false;
        }
      }
    } catch {
      setMicrophoneAccess('granted');
      return true;
    }
  }, []);

  // âœ… ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì • ì‹œì‘ í•¨ìˆ˜
  const startAudioLevelMonitoring = useCallback(() => {
    if (!analyserRef.current) return;
    const bufferLength = analyserRef.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const updateLevel = () => {
      if (!analyserRef.current || isStoppingRef.current) return;
      analyserRef.current.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
      setAudioLevel(Math.min(100, (average / 128) * 100));
    };
    audioLevelTimerRef.current = setInterval(updateLevel, 50);
  }, []);

  // âœ… ë…¹ìŒ ì¤‘ì§€ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í•¨ìˆ˜
  const stopRecording = useCallback(() => {
    if (isStoppingRef.current) return;
    isStoppingRef.current = true;
    if (durationTimerRef.current) clearInterval(durationTimerRef.current);
    if (audioLevelTimerRef.current) clearInterval(audioLevelTimerRef.current);
    speechRecognitionRef.current?.abort();
    mediaRecorderRef.current?.state === 'recording' && mediaRecorderRef.current.stop();
    audioContextRef.current?.close();
    streamRef.current?.getTracks().forEach(track => track.stop());
    streamRef.current = null;
    setIsListening(false);
    setAudioLevel(0);
    onRecordingStop();
    setTimeout(() => { isStoppingRef.current = false; }, 500);
  }, [onRecordingStop]);

  // âœ… Web Speech API ì´ˆê¸°í™” í•¨ìˆ˜
  const initSpeechRecognition = useCallback(() => {
    const SpeechRecognitionConstructor = window.webkitSpeechRecognition || window.SpeechRecognition;
    if (!SpeechRecognitionConstructor) return null;
    try {
      const recognition = new SpeechRecognitionConstructor() as SpeechRecognition;
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'ko-KR';
      recognition.onresult = (event: SpeechRecognitionEvent) => {
        if (isStoppingRef.current) return;
        let finalTranscript = '';
        let interimTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          result.isFinal ? finalTranscript += result[0].transcript : interimTranscript += result[0].transcript;
        }
        const allTranscript = finalTranscript + interimTranscript;
        setTranscript(allTranscript);
        const endKeywords = ['ì´ìƒì…ë‹ˆë‹¤', 'ëì…ë‹ˆë‹¤', 'ë§ˆì¹©ë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ì´ìƒì´ì—ìš”', 'ëì´ì—ìš”', 'ì™„ë£Œì…ë‹ˆë‹¤', 'ë‹¤í–ˆìŠµë‹ˆë‹¤', 'ì´ìƒ', 'ë', 'ì™„ë£Œ', 'ë§ˆì¹¨', 'ë‹µë³€ ë'];
        if (endKeywords.some(keyword => allTranscript.toLowerCase().includes(keyword.toLowerCase())) && finalTranscript.length > 0) {
          setTimeout(() => isListening && stopRecording(), 500);
        }
      };
      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        if (event.error === 'not-allowed') setMicrophoneAccess('denied');
      };
      recognition.onend = () => {
        if (isListening && !isStoppingRef.current && continuousMode) {
          setTimeout(() => speechRecognitionRef.current?.start(), 100);
        }
      };
      return recognition;
    } catch {
      return null;
    }
  }, [isListening, continuousMode, stopRecording]);

  // âœ… MediaRecorder ì´ˆê¸°í™” í•¨ìˆ˜
  const initMediaRecorder = useCallback(async (): Promise<boolean> => {
    const hasPermission = await checkMicrophonePermission();
    if (!hasPermission) return false;
    const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: false } });
    streamRef.current = stream;
    const audioContext = new AudioContext();
    const analyser = audioContext.createAnalyser();
    audioContextRef.current = audioContext;
    analyserRef.current = analyser;
    audioContext.createMediaStreamSource(stream).connect(analyser);
    const mimeType = ['audio/webm;codecs=opus', 'audio/webm', 'audio/wav'].find(type => MediaRecorder.isTypeSupported(type));
    const recorder = new MediaRecorder(stream, mimeType ? { mimeType } : undefined);
    recorder.ondataavailable = e => { if (e.data.size > 0) audioChunksRef.current.push(e.data); };
    recorder.onstop = () => {
      if (audioChunksRef.current.length > 0) {
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType || 'audio/wav' });
        onRecordingComplete(audioBlob);
      }
      audioChunksRef.current = [];
    };
    mediaRecorderRef.current = recorder;
    startAudioLevelMonitoring();
    return true;
  }, [checkMicrophonePermission, onRecordingComplete, startAudioLevelMonitoring]);

  // âœ… ë…¹ìŒ ì‹œê°„ ì¸¡ì • íƒ€ì´ë¨¸
  const startDurationTimer = () => {
    setRecordingDuration(0);
    durationTimerRef.current = setInterval(() => setRecordingDuration(prev => prev + 1), 1000);
  };

  // âœ… ë…¹ìŒ ì‹œì‘ í•¨ìˆ˜
  const startRecording = useCallback(async () => {
    if (isStoppingRef.current || isListening) return;
    setIsProcessing(true);
    const ready = await initMediaRecorder();
    if (!ready) return setIsProcessing(false);
    const recognition = initSpeechRecognition();
    recognition?.start();
    speechRecognitionRef.current = recognition;
    mediaRecorderRef.current?.start(500);
    startDurationTimer();
    setIsListening(true);
    setTranscript('');
    onRecordingStart();
    setIsProcessing(false);
  }, [isListening, initMediaRecorder, initSpeechRecognition, onRecordingStart]);

  // âœ… ë§ˆìš´íŠ¸ ì‹œ ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
  useEffect(() => {
    checkMicrophonePermission();
    return () => { isListening && stopRecording(); };
  }, [isListening]);

  // âœ… isRecording ìƒíƒœ ë³€í™” ê°ì§€ (ì—°ì† ë…¹ìŒìš©)
  useEffect(() => {
    if (continuousMode && isRecording && !isListening && microphoneAccess === 'granted') {
      const timer = setTimeout(() => startRecording(), 1000);
      return () => clearTimeout(timer);
    } else if (!isRecording && isListening) {
      stopRecording();
    }
  }, [isRecording, isListening, microphoneAccess]);

  const formatDuration = (seconds: number): string => `${Math.floor(seconds / 60)}:${(seconds % 60).toString().padStart(2, '0')}`;

  if (microphoneAccess === 'denied') return <div>ğŸ¤ ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤</div>;

  return (
    <div>
      {!continuousMode && (
        <button onClick={isListening ? stopRecording : startRecording}>
          {isListening ? 'â¹ï¸ ë…¹ìŒ ì¤‘ì§€' : 'ğŸ¤ ë…¹ìŒ ì‹œì‘'}
        </button>
      )}
      {isListening && (
        <>
          <div>ğŸ™ï¸ ë…¹ìŒ ì¤‘... {formatDuration(recordingDuration)}</div>
          <AudioVisualizer audioLevel={audioLevel} />
          {transcript && <div>ğŸ’¬ ì¸ì‹: {transcript}</div>}
        </>
      )}
    </div>
  );
};

export default AudioRecorder;
