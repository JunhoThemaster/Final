from fastapi import APIRouter, Depends,HTTPException
from pydantic import BaseModel
import jwt
import datetime
from ....services.JwtUitls import token_utils
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from faster_whisper import WhisperModel
from ....services.audio_module import save_and_transcribe,predict_service
from ....services.websocket import manager as ms
import numpy as np
from scipy.signal import resample_poly
from fastapi import APIRouter, Query
import openai
import librosa
from app.services.auth_service import hash_password


from ....services.interview_generator import InterviewGenerator


from ....auth.jwt_handler import create_access_token,decode_jwt_token
from ....auth.dependencies import get_current_user
from app.models.models import User
from sqlalchemy.orm import Session
import uuid
from app.dependencies import get_db
from passlib.context import CryptContext
from app.models.models import Interview, InterviewVideoAnalyze


def convert_pcm16_bytes_to_float32_array(pcm_bytes: bytes) -> np.ndarray:
    int16_array = np.frombuffer(pcm_bytes, dtype=np.int16)
    float_array = int16_array.astype(np.float32) / 32768.0  # normalize to [-1, 1]
    return float_array
def downsample_to_16k(audio: np.ndarray, orig_sr: int) -> np.ndarray:
    if orig_sr == 16000:
        return audio
    # up/down ë¹„ìœ¨ ê³„ì‚°
    gcd = np.gcd(orig_sr, 16000)
    up = 16000 // gcd
    down = orig_sr // gcd
    return resample_poly(audio, up, down)

import wave
def save_pcm_as_wav(pcm_bytes: bytes, wav_path: str, sample_rate=16000):
    audio_data = np.frombuffer(pcm_bytes, dtype=np.int16)

    with wave.open(wav_path, 'wb') as wf:
        wf.setnchannels(1)          # Mono
        wf.setsampwidth(2)          # 16bit PCM = 2 bytes
        wf.setframerate(sample_rate)
        wf.writeframes(audio_data.tobytes())

        
router = APIRouter(
    prefix="/api/user",  # ì´ ê²½ë¡œê°€ ëª¨ë“  API ì•ì— ë¶™ìŒ
    tags=["user"]        # Swaggerì—ì„œ ë³´ì—¬ì§ˆ ì¹´í…Œê³ ë¦¬ ì´ë¦„
)

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

class LoginData(BaseModel):
    email: str
    password: str

@router.post("/login")
def login(data: LoginData, db: Session = Depends(get_db)):
    # âœ… 1. ì´ë©”ì¼ë¡œ ìœ ì € ì¡°íšŒ
    user = db.query(User).filter(User.email == data.email).first()
    if not user:
        raise HTTPException(status_code=401, detail="ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ìì…ë‹ˆë‹¤.")

    # âœ… 2. ë¹„ë°€ë²ˆí˜¸ ê²€ì¦
    if not pwd_context.verify(data.password, user.password):
        raise HTTPException(status_code=401, detail="ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # âœ… 3. í† í° ìƒì„±
    token = create_access_token(data={"sub": user.name})
    return {"access_token": token, "token_type": "bearer"}



from pydantic import BaseModel

class RegisterRequest(BaseModel):
    username: str
    password: str
    email: str
RegisterRequest.model_rebuild()

@router.post("/Register")
def register_user(request: RegisterRequest,db: Session = Depends(get_db) ):
    # ì´ë©”ì¼ ì¤‘ë³µ ì²´í¬
    if db.query(User).filter(User.email == request.email).first():
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ë“±ë¡ëœ ì´ë©”ì¼ì…ë‹ˆë‹¤.")

    # ì‚¬ìš©ì ìƒì„±
    new_user = User(
        id=str(uuid.uuid4()),
        name=request.username,
        email=request.email,
        password=hash_password(request.password)
    )
    db.add(new_user)
    db.commit()

    return {"message": "íšŒì›ê°€ì… ì„±ê³µ"}




import tempfile
from fastapi import APIRouter, UploadFile, File, WebSocket, HTTPException, Query


from ....services.audio_module.clova_stt import clova_transcribe
from ....services.audio_module.audio_io import load_audio_float32
from ....services.audio_module.audio_convert import convert_webm_to_wav
from ....services.audio_module import predict_service
import tempfile, os
from fastapi import UploadFile, File, Query, HTTPException

from fastapi import Request

import json
from datetime import datetime

SAVE_DIR = "./saved_audios"  # ì›í•˜ëŠ” ì €ì¥ ê²½ë¡œ
from app.models.models import InterviewAudioAnalyze
from fastapi import UploadFile, File, Query, HTTPException
from fastapi import UploadFile, File, Form

@router.post("/audio/{user_id}")
async def audio_analyze(
    user_id: str,         #  ì¸í„°ë·° ID ì¶”ê°€
    question: str = Form(...),             #  ì§ˆë¬¸ ë‚´ìš©
    token: str = Query(...),
    interview_id: str = Form(...),
    audio_file: UploadFile = File(...),
    db: Session = Depends(get_db),         #  DB ì„¸ì…˜ ì£¼ì…
):
    #  í† í° ê²€ì¦
    if not token_utils.verify_token(token):
        raise HTTPException(status_code=401, detail="Unauthorized")

    #  ì €ì¥ ê²½ë¡œ ì¤€ë¹„
    print(user_id)
    user_dir = os.path.join(SAVE_DIR, user_id)
    os.makedirs(user_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    user = db.query(User).filter(User.name == user_id).first()

    print(user_id)
    #  ì˜¤ë””ì˜¤ ì €ì¥
    raw = await audio_file.read()
    content_type = audio_file.content_type or ""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        tmp_path = tmp.name
        if content_type == "audio/wav":
            tmp.write(raw)
        else:
            tmp.close()
            save_pcm_as_wav(raw, tmp_path, sample_rate=16000)

    try:
        #  ë¶„ì„
        text = clova_transcribe(tmp_path)
        # ëª¨ë¸ì— ë“¤ì–´ê°€ê¸°ì „ ì „ì²˜ë¦¬
        y = load_audio_float32(tmp_path) 
        emotion, probs = predict_service.predict_emotion(y)
    finally:
        os.remove(tmp_path)

    # softmax í™•ë¥  ë§µ
    label_classes = np.load("app/services/audio_module/label_encoder_classes.npy", allow_pickle=True)
    probs_map = {cls: float(p) for cls, p in zip(label_classes, probs)}
    #  DB ì €ì¥
    analysis = InterviewAudioAnalyze(
        interview_id=interview_id,
        user_id=user.id,
        timestamp=datetime.utcnow(),
        question=question,
        answer=text.strip(),
        emotion=emotion,
        probabilities=probs_map
    )
    db.add(analysis)
    db.commit()

    #  íŒŒì¼ë„ ë°±ì—…ìš© ì €ì¥ 
    result = {
        "user_id": user.id,
        "interview_id": interview_id,
        "timestamp": timestamp,
        "question": question,
        "text": text.strip(),
        "emotion": emotion,
        "probabilities": probs_map,
    }
    json_path = os.path.join(user_dir, f"{timestamp}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f" ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
    return result

class FieldRequest(BaseModel):
    field: str


interview_generator = InterviewGenerator()



from pydantic import BaseModel
from app.models.models import Interview
from uuid import uuid4

class InterviewSetupRequest(BaseModel):
    jobUrl: str

@router.post("/interview/setup/{cate}/{n_q}")
async def setup_interview(
    cate: str,
    n_q: int,
    req: InterviewSetupRequest,
    user=Depends(get_current_user),
    db: Session = Depends(get_db),  #  DB ì„¸ì…˜ ì¶”ê°€
):
    

    """ë©´ì ‘ ì„¸ì…˜ ì„¤ì •"""
    try:
       
        job_url = req.jobUrl
        interview_id = str(uuid4())  #  ì¸í„°ë·° UUID ìƒì„±

        print(f" jobUrl ìˆ˜ì‹ ë¨: {job_url}")

        #  1. ì§ˆë¬¸ ìƒì„±
        questions = await interview_generator.generate_questions(
            cate,
            job_url,
            n_q
        )
        user = db.query(User).filter(User.name == user['sub']).first()
        # ì¸í„°ë·° ê°ì²´ ìƒì„± ë° ì €ì¥
        new_interview = Interview(
            id=interview_id,
            user_id=user.id,
            job_position=cate,
            job_url=job_url
        )
        db.add(new_interview)
        db.commit()
        
        return {
            "interview_id" : new_interview.id,
            "user_id": user.id,
            "questions": questions,
            "job_position": cate,
            "job_url": job_url,
            
            "message": "ë©´ì ‘ ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        print(f" ë©´ì ‘ ì„¤ì • ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë©´ì ‘ ì„¤ì • ì˜¤ë¥˜: {str(e)}")



@router.get("/interview/{interview_id}/analysis")
async def get_combined_analysis(interview_id: str, db: Session = Depends(get_db)):
    print(f"ğŸ” ë¶„ì„ ìš”ì²­ë¨ - ì¸í„°ë·° ID: {interview_id}")

    interview = db.query(Interview).filter_by(id=interview_id).first()
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found")

    audio_data = db.query(InterviewAudioAnalyze).filter_by(interview_id=interview_id).all()
    video_data = db.query(InterviewVideoAnalyze).filter_by(interview_id=interview_id).all()



    # ğŸ”¸ ì‹œê°í™”ìš© í†µê³„ ë°ì´í„° ìƒì„±
    try:
        visual_analysis = extract_visual_data(audio_data)
        video_analysis = extract_video_visual_data(video_data)
        print(visual_analysis)
        print(video_analysis)
    except Exception as e:
        print("âŒ ì‹œê°í™” í†µê³„ ì²˜ë¦¬ ì‹¤íŒ¨:", e)
        visual_analysis = None
        video_analysis =None
    # ğŸ”¸ LLM ê°ì • ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ìš”ì²­ (ìŒì„± í…ìŠ¤íŠ¸)

    try:
        audio_prompt = await interview_generator.build_audio_prompt(audio_data, interview.job_position, interview.job_url)
        audio_summary = await interview_generator.call_llm(audio_prompt)
        print("âœ… LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        print("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨:", e)
        audio_summary = None


    try:
        video_prompt = await interview_generator.build_video_prompt(video_data, interview.job_position, interview.job_url)
        video_summary = await interview_generator.call_llm(video_prompt)
        print("âœ… LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        print("âŒ LLM ì˜ìƒ í˜¸ì¶œ ì‹¤íŒ¨:", e)
        video_summary = None
    try:
        audio_serialized = [serialize_audio_row(r) for r in audio_data]
        video_serialized = [serialize_video_row(r) for r in video_data]
        combined_prompt = await interview_generator.build_comprehensive_interview_feedback(
            audio_serialized, video_serialized, interview.job_position, interview.job_url
        )
        combined_summary = await interview_generator.call_llm(combined_prompt)
        print("âœ… í†µí•© í”¼ë“œë°± ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        print("âŒ í†µí•© LLM í˜¸ì¶œ ì‹¤íŒ¨:", e)
        combined_summary = None
    return {
        "audio_summary": audio_summary,
        "video_summary": video_summary,
        "audio_visual_analysis": visual_analysis,
        "video_visual_analysis": video_analysis,
        "final_feedback": combined_summary
    }
    


import pandas as pd
from collections import Counter
import json

def extract_visual_data(audio_data):
    records = []

    for r in audio_data:
        try:
            probs = r.probabilities if isinstance(r.probabilities, dict) else {}
        except Exception:
            probs = {}

        records.append({
            "question": r.question,
            "emotion": r.emotion,
            **probs  # ì „ì²´ ê°ì • í™•ë¥  ì»¬ëŸ¼ìœ¼ë¡œ í™•ì¥
        })

    df = pd.DataFrame(records)

    # â— ê°ì •ë³„ í™•ë¥ ê°’ì´ ì—†ëŠ” ì»¬ëŸ¼ë“¤ë„ ë¯¸ë¦¬ ì±„ì›Œë„£ê¸°
    all_emotions = ["fear", "angry", "disgust", "neutral", "sadness", "surprise", "happiness"]
    for emotion in all_emotions:
        if emotion not in df.columns:
            df[emotion] = 0.0

    df.fillna(0, inplace=True)  # í˜¹ì‹œ ëª¨ë¥¼ NaNë„ ì œê±°

    # ğŸ¯ ì‹œê°í™”ìš© raw ë°ì´í„°
    visual_data = df.to_dict(orient='records')

    # ğŸ“ˆ í‰ê·  í™•ë¥  ê³„ì‚°
    average_emotion_scores = df[all_emotions].mean().round(4).to_dict()

    # ğŸ” ìµœë¹ˆ ê°ì •
    emotion_counts = Counter(df['emotion'])
    most_common_emotion, top_count = emotion_counts.most_common(1)[0]
    top_emotion_ratio = round(top_count / len(df), 4)

    # ğŸ“Œ ìš”ì•½ í†µê³„
    summary_stats = {
        "num_questions": len(df),
        "unique_emotions": df['emotion'].nunique(),
        "top_emotion_ratio": top_emotion_ratio
    }

    return {
        "visual_data": visual_data,
        "average_emotion_scores": average_emotion_scores,
        "most_common_emotion": most_common_emotion,
        "summary_stats": summary_stats
    }

def extract_video_visual_data(video_data):
    import pandas as pd
    from collections import Counter

    records = []
    for r in video_data:
        records.append({
            "timestamp": r.timestamp,
            "emotion": r.emotion,
            "raw_emotion": r.raw_emotion,
            "confidence": r.confidence,
            "blink_count": r.blink_count,
            "posture": r.posture,
            "gaze_x": r.gaze_x,
            "gaze_y": r.gaze_y,
            "ear": r.ear,
            "head_x": r.head_pose[0] if r.head_pose else None,
            "head_y": r.head_pose[1] if r.head_pose else None,
            "head_z": r.head_pose[2] if r.head_pose else None,
        })

    df = pd.DataFrame(records)
    df.fillna(0, inplace=True)

    # ğŸ“ˆ ê°ì • ìš”ì•½
    emotion_counts = Counter(df['emotion'])
    most_common_emotion, top_count = emotion_counts.most_common(1)[0]
    top_emotion_ratio = round(top_count / len(df), 4)

    # ğŸ“ˆ í‰ê·  ìˆ˜ì¹˜ ê³„ì‚°
    numeric_cols = ["confidence", "blink_count", "gaze_x", "gaze_y", "ear", "head_x", "head_y", "head_z"]
    averages = df[numeric_cols].mean().round(4).to_dict()

    # ğŸ“Š ìì„¸ í†µê³„
    posture_counts = df['posture'].value_counts().to_dict()
    posture_ratio = {k: round(v / len(df), 4) for k, v in posture_counts.items()}

    # ğŸ“Œ ìš”ì•½ í†µê³„
    summary_stats = {
        "num_frames": len(df),
        "unique_emotions": df['emotion'].nunique(),
        "top_emotion_ratio": top_emotion_ratio,
        "posture_distribution": posture_ratio,
    }

    return {
        
        "average_metrics": averages,
        "most_common_emotion": most_common_emotion,
        "summary_stats": summary_stats
    }
def serialize_audio_row(row):
    return {
        "timestamp": str(row.timestamp),
        "text": row.answer,
        "emotion": row.emotion,
        "probabilities": row.probabilities  # ì´ë¯¸ JSONì´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
    }

def serialize_video_row(row):
    return {
        "timestamp": str(row.timestamp),
        "emotion": row.emotion,
        "confidence": row.confidence,
        "posture": row.posture,
        "blink_count": row.blink_count,
        "gaze_x": row.gaze_x,
        "gaze_y": row.gaze_y,
        "ear": row.ear,
        "head_pose":  row.head_pose
    }


